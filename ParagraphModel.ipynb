{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bf5cbae-6dd6-4b22-a0f8-fdb79d6708ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import datasets\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8879ecd-d627-4e1a-8d22-766c7d28f0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-cde8372ac5f3dd32\n",
      "Reusing dataset csv (/home/dnarendr/.cache/huggingface/datasets/csv/default-cde8372ac5f3dd32/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d23639484b0e422d97a588013df8d539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('csv', data_files={'train': ['para_train.csv'], 'test': 'para_test.csv', 'validation':'para_validate.csv'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61767a14-4332-4d42-9541-c10aad89d8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    try:\n",
    "      return tokenizer(example[\"text\"], truncation=True)\n",
    "    except  Exception as e:\n",
    "      print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b0cf1d6-1734-48ed-8b7c-2099eb6e7019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b95e9ba04154398941feca00a1aded1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4667c3ee66340e8ae2179ff5180dcb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79eb66d2ec3e4b808903a06fe1b157de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized_datasets:: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 2027\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['label', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 408\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['label', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 272\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "print(\"tokenized_datasets::\", tokenized_datasets)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1368a1c-f653-47fd-a217-aadccd7ee23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"paraModel\", evaluation_strategy=\"steps\", eval_steps=100, num_train_epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf2a5008-d45f-4414-a099-781bd65637ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c53efd1-10a2-40dd-9630-46812d2f29f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):    \n",
    "    pred, labels = eval_pred\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "    recall = recall_score(y_true=labels, y_pred=pred, average='micro')\n",
    "    precision = precision_score(y_true=labels, y_pred=pred, average='micro')\n",
    "    f1 = f1_score(y_true=labels, y_pred=pred, average='micro')\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5dba785-b140-40ef-aa31-cfff04c760b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/dnarendr/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\",\n",
      "    \"43\": \"LABEL_43\",\n",
      "    \"44\": \"LABEL_44\",\n",
      "    \"45\": \"LABEL_45\",\n",
      "    \"46\": \"LABEL_46\",\n",
      "    \"47\": \"LABEL_47\",\n",
      "    \"48\": \"LABEL_48\",\n",
      "    \"49\": \"LABEL_49\",\n",
      "    \"50\": \"LABEL_50\",\n",
      "    \"51\": \"LABEL_51\",\n",
      "    \"52\": \"LABEL_52\",\n",
      "    \"53\": \"LABEL_53\",\n",
      "    \"54\": \"LABEL_54\",\n",
      "    \"55\": \"LABEL_55\",\n",
      "    \"56\": \"LABEL_56\",\n",
      "    \"57\": \"LABEL_57\",\n",
      "    \"58\": \"LABEL_58\",\n",
      "    \"59\": \"LABEL_59\",\n",
      "    \"60\": \"LABEL_60\",\n",
      "    \"61\": \"LABEL_61\",\n",
      "    \"62\": \"LABEL_62\",\n",
      "    \"63\": \"LABEL_63\",\n",
      "    \"64\": \"LABEL_64\",\n",
      "    \"65\": \"LABEL_65\",\n",
      "    \"66\": \"LABEL_66\",\n",
      "    \"67\": \"LABEL_67\",\n",
      "    \"68\": \"LABEL_68\",\n",
      "    \"69\": \"LABEL_69\",\n",
      "    \"70\": \"LABEL_70\",\n",
      "    \"71\": \"LABEL_71\",\n",
      "    \"72\": \"LABEL_72\",\n",
      "    \"73\": \"LABEL_73\",\n",
      "    \"74\": \"LABEL_74\",\n",
      "    \"75\": \"LABEL_75\",\n",
      "    \"76\": \"LABEL_76\",\n",
      "    \"77\": \"LABEL_77\",\n",
      "    \"78\": \"LABEL_78\",\n",
      "    \"79\": \"LABEL_79\",\n",
      "    \"80\": \"LABEL_80\",\n",
      "    \"81\": \"LABEL_81\",\n",
      "    \"82\": \"LABEL_82\",\n",
      "    \"83\": \"LABEL_83\",\n",
      "    \"84\": \"LABEL_84\",\n",
      "    \"85\": \"LABEL_85\",\n",
      "    \"86\": \"LABEL_86\",\n",
      "    \"87\": \"LABEL_87\",\n",
      "    \"88\": \"LABEL_88\",\n",
      "    \"89\": \"LABEL_89\",\n",
      "    \"90\": \"LABEL_90\",\n",
      "    \"91\": \"LABEL_91\",\n",
      "    \"92\": \"LABEL_92\",\n",
      "    \"93\": \"LABEL_93\",\n",
      "    \"94\": \"LABEL_94\",\n",
      "    \"95\": \"LABEL_95\",\n",
      "    \"96\": \"LABEL_96\",\n",
      "    \"97\": \"LABEL_97\",\n",
      "    \"98\": \"LABEL_98\",\n",
      "    \"99\": \"LABEL_99\",\n",
      "    \"100\": \"LABEL_100\",\n",
      "    \"101\": \"LABEL_101\",\n",
      "    \"102\": \"LABEL_102\",\n",
      "    \"103\": \"LABEL_103\",\n",
      "    \"104\": \"LABEL_104\",\n",
      "    \"105\": \"LABEL_105\",\n",
      "    \"106\": \"LABEL_106\",\n",
      "    \"107\": \"LABEL_107\",\n",
      "    \"108\": \"LABEL_108\",\n",
      "    \"109\": \"LABEL_109\",\n",
      "    \"110\": \"LABEL_110\",\n",
      "    \"111\": \"LABEL_111\",\n",
      "    \"112\": \"LABEL_112\",\n",
      "    \"113\": \"LABEL_113\",\n",
      "    \"114\": \"LABEL_114\",\n",
      "    \"115\": \"LABEL_115\",\n",
      "    \"116\": \"LABEL_116\",\n",
      "    \"117\": \"LABEL_117\",\n",
      "    \"118\": \"LABEL_118\",\n",
      "    \"119\": \"LABEL_119\",\n",
      "    \"120\": \"LABEL_120\",\n",
      "    \"121\": \"LABEL_121\",\n",
      "    \"122\": \"LABEL_122\",\n",
      "    \"123\": \"LABEL_123\",\n",
      "    \"124\": \"LABEL_124\",\n",
      "    \"125\": \"LABEL_125\",\n",
      "    \"126\": \"LABEL_126\",\n",
      "    \"127\": \"LABEL_127\",\n",
      "    \"128\": \"LABEL_128\",\n",
      "    \"129\": \"LABEL_129\",\n",
      "    \"130\": \"LABEL_130\",\n",
      "    \"131\": \"LABEL_131\",\n",
      "    \"132\": \"LABEL_132\",\n",
      "    \"133\": \"LABEL_133\",\n",
      "    \"134\": \"LABEL_134\",\n",
      "    \"135\": \"LABEL_135\",\n",
      "    \"136\": \"LABEL_136\",\n",
      "    \"137\": \"LABEL_137\",\n",
      "    \"138\": \"LABEL_138\",\n",
      "    \"139\": \"LABEL_139\",\n",
      "    \"140\": \"LABEL_140\",\n",
      "    \"141\": \"LABEL_141\",\n",
      "    \"142\": \"LABEL_142\",\n",
      "    \"143\": \"LABEL_143\",\n",
      "    \"144\": \"LABEL_144\",\n",
      "    \"145\": \"LABEL_145\",\n",
      "    \"146\": \"LABEL_146\",\n",
      "    \"147\": \"LABEL_147\",\n",
      "    \"148\": \"LABEL_148\",\n",
      "    \"149\": \"LABEL_149\",\n",
      "    \"150\": \"LABEL_150\",\n",
      "    \"151\": \"LABEL_151\",\n",
      "    \"152\": \"LABEL_152\",\n",
      "    \"153\": \"LABEL_153\",\n",
      "    \"154\": \"LABEL_154\",\n",
      "    \"155\": \"LABEL_155\",\n",
      "    \"156\": \"LABEL_156\",\n",
      "    \"157\": \"LABEL_157\",\n",
      "    \"158\": \"LABEL_158\",\n",
      "    \"159\": \"LABEL_159\",\n",
      "    \"160\": \"LABEL_160\",\n",
      "    \"161\": \"LABEL_161\",\n",
      "    \"162\": \"LABEL_162\",\n",
      "    \"163\": \"LABEL_163\",\n",
      "    \"164\": \"LABEL_164\",\n",
      "    \"165\": \"LABEL_165\",\n",
      "    \"166\": \"LABEL_166\",\n",
      "    \"167\": \"LABEL_167\",\n",
      "    \"168\": \"LABEL_168\",\n",
      "    \"169\": \"LABEL_169\",\n",
      "    \"170\": \"LABEL_170\",\n",
      "    \"171\": \"LABEL_171\",\n",
      "    \"172\": \"LABEL_172\",\n",
      "    \"173\": \"LABEL_173\",\n",
      "    \"174\": \"LABEL_174\",\n",
      "    \"175\": \"LABEL_175\",\n",
      "    \"176\": \"LABEL_176\",\n",
      "    \"177\": \"LABEL_177\",\n",
      "    \"178\": \"LABEL_178\",\n",
      "    \"179\": \"LABEL_179\",\n",
      "    \"180\": \"LABEL_180\",\n",
      "    \"181\": \"LABEL_181\",\n",
      "    \"182\": \"LABEL_182\",\n",
      "    \"183\": \"LABEL_183\",\n",
      "    \"184\": \"LABEL_184\",\n",
      "    \"185\": \"LABEL_185\",\n",
      "    \"186\": \"LABEL_186\",\n",
      "    \"187\": \"LABEL_187\",\n",
      "    \"188\": \"LABEL_188\",\n",
      "    \"189\": \"LABEL_189\",\n",
      "    \"190\": \"LABEL_190\",\n",
      "    \"191\": \"LABEL_191\",\n",
      "    \"192\": \"LABEL_192\",\n",
      "    \"193\": \"LABEL_193\",\n",
      "    \"194\": \"LABEL_194\",\n",
      "    \"195\": \"LABEL_195\",\n",
      "    \"196\": \"LABEL_196\",\n",
      "    \"197\": \"LABEL_197\",\n",
      "    \"198\": \"LABEL_198\",\n",
      "    \"199\": \"LABEL_199\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_100\": 100,\n",
      "    \"LABEL_101\": 101,\n",
      "    \"LABEL_102\": 102,\n",
      "    \"LABEL_103\": 103,\n",
      "    \"LABEL_104\": 104,\n",
      "    \"LABEL_105\": 105,\n",
      "    \"LABEL_106\": 106,\n",
      "    \"LABEL_107\": 107,\n",
      "    \"LABEL_108\": 108,\n",
      "    \"LABEL_109\": 109,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_110\": 110,\n",
      "    \"LABEL_111\": 111,\n",
      "    \"LABEL_112\": 112,\n",
      "    \"LABEL_113\": 113,\n",
      "    \"LABEL_114\": 114,\n",
      "    \"LABEL_115\": 115,\n",
      "    \"LABEL_116\": 116,\n",
      "    \"LABEL_117\": 117,\n",
      "    \"LABEL_118\": 118,\n",
      "    \"LABEL_119\": 119,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_120\": 120,\n",
      "    \"LABEL_121\": 121,\n",
      "    \"LABEL_122\": 122,\n",
      "    \"LABEL_123\": 123,\n",
      "    \"LABEL_124\": 124,\n",
      "    \"LABEL_125\": 125,\n",
      "    \"LABEL_126\": 126,\n",
      "    \"LABEL_127\": 127,\n",
      "    \"LABEL_128\": 128,\n",
      "    \"LABEL_129\": 129,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_130\": 130,\n",
      "    \"LABEL_131\": 131,\n",
      "    \"LABEL_132\": 132,\n",
      "    \"LABEL_133\": 133,\n",
      "    \"LABEL_134\": 134,\n",
      "    \"LABEL_135\": 135,\n",
      "    \"LABEL_136\": 136,\n",
      "    \"LABEL_137\": 137,\n",
      "    \"LABEL_138\": 138,\n",
      "    \"LABEL_139\": 139,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_140\": 140,\n",
      "    \"LABEL_141\": 141,\n",
      "    \"LABEL_142\": 142,\n",
      "    \"LABEL_143\": 143,\n",
      "    \"LABEL_144\": 144,\n",
      "    \"LABEL_145\": 145,\n",
      "    \"LABEL_146\": 146,\n",
      "    \"LABEL_147\": 147,\n",
      "    \"LABEL_148\": 148,\n",
      "    \"LABEL_149\": 149,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_150\": 150,\n",
      "    \"LABEL_151\": 151,\n",
      "    \"LABEL_152\": 152,\n",
      "    \"LABEL_153\": 153,\n",
      "    \"LABEL_154\": 154,\n",
      "    \"LABEL_155\": 155,\n",
      "    \"LABEL_156\": 156,\n",
      "    \"LABEL_157\": 157,\n",
      "    \"LABEL_158\": 158,\n",
      "    \"LABEL_159\": 159,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_160\": 160,\n",
      "    \"LABEL_161\": 161,\n",
      "    \"LABEL_162\": 162,\n",
      "    \"LABEL_163\": 163,\n",
      "    \"LABEL_164\": 164,\n",
      "    \"LABEL_165\": 165,\n",
      "    \"LABEL_166\": 166,\n",
      "    \"LABEL_167\": 167,\n",
      "    \"LABEL_168\": 168,\n",
      "    \"LABEL_169\": 169,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_170\": 170,\n",
      "    \"LABEL_171\": 171,\n",
      "    \"LABEL_172\": 172,\n",
      "    \"LABEL_173\": 173,\n",
      "    \"LABEL_174\": 174,\n",
      "    \"LABEL_175\": 175,\n",
      "    \"LABEL_176\": 176,\n",
      "    \"LABEL_177\": 177,\n",
      "    \"LABEL_178\": 178,\n",
      "    \"LABEL_179\": 179,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_180\": 180,\n",
      "    \"LABEL_181\": 181,\n",
      "    \"LABEL_182\": 182,\n",
      "    \"LABEL_183\": 183,\n",
      "    \"LABEL_184\": 184,\n",
      "    \"LABEL_185\": 185,\n",
      "    \"LABEL_186\": 186,\n",
      "    \"LABEL_187\": 187,\n",
      "    \"LABEL_188\": 188,\n",
      "    \"LABEL_189\": 189,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_190\": 190,\n",
      "    \"LABEL_191\": 191,\n",
      "    \"LABEL_192\": 192,\n",
      "    \"LABEL_193\": 193,\n",
      "    \"LABEL_194\": 194,\n",
      "    \"LABEL_195\": 195,\n",
      "    \"LABEL_196\": 196,\n",
      "    \"LABEL_197\": 197,\n",
      "    \"LABEL_198\": 198,\n",
      "    \"LABEL_199\": 199,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_43\": 43,\n",
      "    \"LABEL_44\": 44,\n",
      "    \"LABEL_45\": 45,\n",
      "    \"LABEL_46\": 46,\n",
      "    \"LABEL_47\": 47,\n",
      "    \"LABEL_48\": 48,\n",
      "    \"LABEL_49\": 49,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_50\": 50,\n",
      "    \"LABEL_51\": 51,\n",
      "    \"LABEL_52\": 52,\n",
      "    \"LABEL_53\": 53,\n",
      "    \"LABEL_54\": 54,\n",
      "    \"LABEL_55\": 55,\n",
      "    \"LABEL_56\": 56,\n",
      "    \"LABEL_57\": 57,\n",
      "    \"LABEL_58\": 58,\n",
      "    \"LABEL_59\": 59,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_60\": 60,\n",
      "    \"LABEL_61\": 61,\n",
      "    \"LABEL_62\": 62,\n",
      "    \"LABEL_63\": 63,\n",
      "    \"LABEL_64\": 64,\n",
      "    \"LABEL_65\": 65,\n",
      "    \"LABEL_66\": 66,\n",
      "    \"LABEL_67\": 67,\n",
      "    \"LABEL_68\": 68,\n",
      "    \"LABEL_69\": 69,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_70\": 70,\n",
      "    \"LABEL_71\": 71,\n",
      "    \"LABEL_72\": 72,\n",
      "    \"LABEL_73\": 73,\n",
      "    \"LABEL_74\": 74,\n",
      "    \"LABEL_75\": 75,\n",
      "    \"LABEL_76\": 76,\n",
      "    \"LABEL_77\": 77,\n",
      "    \"LABEL_78\": 78,\n",
      "    \"LABEL_79\": 79,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_80\": 80,\n",
      "    \"LABEL_81\": 81,\n",
      "    \"LABEL_82\": 82,\n",
      "    \"LABEL_83\": 83,\n",
      "    \"LABEL_84\": 84,\n",
      "    \"LABEL_85\": 85,\n",
      "    \"LABEL_86\": 86,\n",
      "    \"LABEL_87\": 87,\n",
      "    \"LABEL_88\": 88,\n",
      "    \"LABEL_89\": 89,\n",
      "    \"LABEL_9\": 9,\n",
      "    \"LABEL_90\": 90,\n",
      "    \"LABEL_91\": 91,\n",
      "    \"LABEL_92\": 92,\n",
      "    \"LABEL_93\": 93,\n",
      "    \"LABEL_94\": 94,\n",
      "    \"LABEL_95\": 95,\n",
      "    \"LABEL_96\": 96,\n",
      "    \"LABEL_97\": 97,\n",
      "    \"LABEL_98\": 98,\n",
      "    \"LABEL_99\": 99\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/dnarendr/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2b63a92-7590-485c-9def-5c36fe5abaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8149174e-2757-423f-a5d6-cdee0b252628",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/dnarendr/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2027\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 762\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='762' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  3/762 00:01 < 22:48, 0.55 it/s, Epoch 0.01/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "Target 130 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/trainer.py:1400\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1398\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1399\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1400\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1403\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1405\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1406\u001b[0m ):\n\u001b[1;32m   1407\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1408\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/trainer.py:1984\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1981\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   1983\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautocast_smart_context_manager():\n\u001b[0;32m-> 1984\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1986\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1987\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/trainer.py:2016\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2014\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2015\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2016\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2017\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2018\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:1580\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1578\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mproblem_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle_label_classification\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1579\u001b[0m     loss_fct \u001b[38;5;241m=\u001b[39m CrossEntropyLoss()\n\u001b[0;32m-> 1580\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1581\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mproblem_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulti_label_classification\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1582\u001b[0m     loss_fct \u001b[38;5;241m=\u001b[39m BCEWithLogitsLoss()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py:1163\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py:2996\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   2994\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2995\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 2996\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: Target 130 is out of bounds."
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aaae99fe-0f8e-4f7f-bae4-be3898dcff10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in my_model/config.json\n",
      "Model weights saved in my_model/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(\"my_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5498870-700f-4ac5-99da-3c9aa70939ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file my_model/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"my_model\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file my_model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at my_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('my_model', local_files_only=True)\n",
    "classifier = pipeline(task = 'text-classification', model = model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddfdbf58-b981-4212-80af-57060cef251c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Disabling tokenizer parallelism, we're using DataLoader multithreading already\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_1', 'score': 0.9908129572868347},\n",
       " {'label': 'LABEL_1', 'score': 0.9354974031448364},\n",
       " {'label': 'LABEL_0', 'score': 0.7992977499961853},\n",
       " {'label': 'LABEL_1', 'score': 0.8896874189376831}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier([\"Your Zions First National Bank Card has been issued by Zions Bancorporation, NA dba Zions First National Bank.\", \"Your Account is with Zions Bancorporation, NA (Bank) and will be administered by the Banks Bankcard Services department.\", \"Your Credit Card, monthly statement, and other associated materials will bear the Zions First National Bank, National Bank of Arizona, Nevada State Bank, or Vectra Bank Colorado name.\", \"The Card is the property of the Bank.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9438f7e-af33-4edc-bb71-eb76eb4c483a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1679\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='210' max='210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [210/210 02:32]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_predictions = trainer.predict(tokenized_datasets['test'])\n",
    "import numpy as np\n",
    "\n",
    "preds = np.argmax(model_predictions.predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d3ba7126-6731-4aea-ab5a-274cda4ab731",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m final_score \u001b[38;5;241m=\u001b[39m \u001b[43mmetric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_predictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreferences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_predictions.test[1].label\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/datasets/metric.py:419\u001b[0m, in \u001b[0;36mMetric.compute\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    416\u001b[0m compute_kwargs \u001b[38;5;241m=\u001b[39m {k: kwargs[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures}\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m--> 419\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_finalize()\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/datasets/metric.py:463\u001b[0m, in \u001b[0;36mMetric.add_batch\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    461\u001b[0m batch \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m\"\u001b[39m: predictions, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreferences\u001b[39m\u001b[38;5;124m\"\u001b[39m: references, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[1;32m    462\u001b[0m batch \u001b[38;5;241m=\u001b[39m {intput_name: batch[intput_name] \u001b[38;5;28;01mfor\u001b[39;00m intput_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures}\n\u001b[0;32m--> 463\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_writer()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/datasets/features/features.py:1313\u001b[0m, in \u001b[0;36mFeatures.encode_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m   1311\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, column \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   1312\u001b[0m     column \u001b[38;5;241m=\u001b[39m cast_to_python_objects(column)\n\u001b[0;32m-> 1313\u001b[0m     encoded_batch[key] \u001b[38;5;241m=\u001b[39m [encode_nested_example(\u001b[38;5;28mself\u001b[39m[key], obj) \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m column]\n\u001b[1;32m   1314\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m encoded_batch\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "final_score = metric.compute(model_predictions=preds, references='model_predictions.label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800965be-4a64-481a-b717-1b4ec203dc8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cf462f-7f0d-4001-b5d5-f01ac68bbe04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc-autonumbering": true,
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
